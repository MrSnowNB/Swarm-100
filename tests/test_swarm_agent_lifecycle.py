#!/usr/bin/env python3
"""
---
file: test_swarm_agent_lifecycle.py
purpose: Comprehensive test suite for agent lifecycle management
framework: pytest with pytest-benchmark and robot framework integration
generated by: Grok Code Generation
status: generated for implementation validation
created: 2025-10-18
---
"""

import pytest
import time
import subprocess
import threading
import os
import yaml
import signal
import psutil
from concurrent.futures import ThreadPoolExecutor
import requests
from typing import List, Dict, Optional, cast, Any
import logging


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AgentLifecycleSimulator:
    """Simulator for testing agent lifecycle management"""

    def __init__(self, config_path='configs/swarm_config.yaml'):
        with open(config_path, 'r') as f:
            raw_config = yaml.safe_load(f)
        if raw_config is None:
            raise ValueError(f"Configuration file '{config_path}' is empty or invalid")
        self.config: Dict[str, Any] = cast(Dict[str, Any], raw_config)
        self.launched_processes: List[subprocess.Popen] = []
        self.agent_configs: List[Dict[str, Any]] = []
        self.base_port = self.config['bot']['base_port']

    def generate_agent_config(self, gpu_id: int, agent_id: int) -> Dict[str, Any]:
        """Generate configuration for a single agent"""
        port = self.base_port + (gpu_id * 100) + agent_id
        config = {
            'bot_id': f"bot_{gpu_id:02d}_{agent_id:02d}",
            'gpu_id': gpu_id,
            'port': port,
            'env': {
                'CUDA_VISIBLE_DEVICES': str(gpu_id),
                'BOT_ID': f"bot_{gpu_id}_{agent_id}",
                'GPU_ID': str(gpu_id),
                'BOT_PORT': str(port)
            }
        }
        return config

    def launch_single_agent(self, gpu_id: int, agent_id: int) -> Optional[Dict[str, Any]]:
        """Launch a single agent process for testing"""
        config = self.generate_agent_config(gpu_id, agent_id)

        # Launch bot worker
        cmd = ['python3', 'scripts/bot_worker.py',
               '--bot-id', config['bot_id'],
               '--gpu', str(config['gpu_id']),
               '--port', config['port']]

        try:
            proc = subprocess.Popen(
                cmd,
                env={**os.environ, **config['env']},
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )

            config['process'] = proc
            config['launch_time'] = time.time()
            self.launched_processes.append(proc)
            self.agent_configs.append(config)

            # Wait for agent to initialize
            time.sleep(2)

            return config

        except Exception as e:
            logger.error(f"Failed to launch agent {config['bot_id']}: {e}")
            return None

    def check_agent_health(self, config: Dict[str, Any]) -> bool:
        """Check if agent is healthy by querying its health endpoint"""
        # Since agents don't have direct health endpoints, check process status
        if config.get('process') and config['process'].poll() is None:
            # Process is running
            return True
        return False

    def stop_agent(self, config: Dict[str, Any]) -> bool:
        """Stop a single agent gracefully"""
        if config.get('process'):
            try:
                config['process'].terminate()
                # Wait up to 10 seconds for graceful shutdown
                config['process'].wait(timeout=10)
                return True
            except subprocess.TimeoutExpired:
                # Force kill if not responding
                config['process'].kill()
                config['process'].wait()
                return True
        return False

    def cleanup_all_agents(self):
        """Clean up all launched agents"""
        for proc in self.launched_processes:
            try:
                if proc.poll() is None:
                    proc.terminate()
                    proc.wait(timeout=5)
            except:
                try:
                    proc.kill()
                    proc.wait()
                except:
                    pass
        self.launched_processes.clear()
        self.agent_configs.clear()


class LifecycleMetrics:
    """Metrics collection for agent lifecycle operations"""

    def __init__(self):
        self.metrics = {
            'launch_times': [],
            'shutdown_times': [],
            'health_check_latencies': [],
            'memory_footprints': [],
            'restart_count': 0
        }

    def record_launch_time(self, time_taken: float):
        self.metrics['launch_times'].append(time_taken)

    def record_shutdown_time(self, time_taken: float):
        self.metrics['shutdown_times'].append(time_taken)

    def record_health_check_latency(self, latency: float):
        self.metrics['health_check_latencies'].append(latency)

    def record_memory_footprint(self, memory_mb: float):
        self.metrics['memory_footprints'].append(memory_mb)

    def increment_restart_count(self):
        self.metrics['restart_count'] += 1


@pytest.fixture(scope="module")
def agent_lifecycle_simulator():
    """Fixture for agent lifecycle testing"""
    simulator = AgentLifecycleSimulator()
    yield simulator
    # Cleanup after tests
    simulator.cleanup_all_agents()


@pytest.fixture
def metrics_collector():
    """Fixture for collecting lifecycle metrics"""
    return LifecycleMetrics()


# Test suite for agent lifecycle management
# 100% green requirement - all tests must pass before proceeding with swarm optimization

@pytest.mark.lifecycle
def test_agent_launch_time_validation(agent_lifecycle_simulator, metrics_collector, benchmark):
    """Test 1: Agent launch time must be <100ms (p99 percentile)"""
    def launch_and_measure():
        start_time = time.time()
        agent = agent_lifecycle_simulator.launch_single_agent(0, 0)
        launch_time = time.time() - start_time
        metrics_collector.record_launch_time(launch_time)

        assert agent is not None, "Agent launch failed"
        assert agent_lifecycle_simulator.check_agent_health(agent), "Agent not healthy after launch"
        assert launch_time < 0.1, f"Launch time too slow: {launch_time:.3f}s"  # 100ms requirement

    # Run launch test 10 times for statistical significance
    for _ in range(10):
        launch_and_measure()

    # Validate p99 requirement
    launch_times = metrics_collector.metrics['launch_times']
    if len(launch_times) >= 9:  # Need 10 samples for p99
        p99_launch_time = sorted(launch_times)[8]  # 9th element is p99 for 10 samples
        assert p99_launch_time < 0.1, f"P99 launch time violation: {p99_launch_time:.3f}s"

    logger.info(f"Average launch time: {sum(launch_times)/len(launch_times):.3f}s")


@pytest.mark.lifecycle
def test_agent_shutdown_graceful(agent_lifecycle_simulator, metrics_collector):
    """Test 2: Agents must shut down gracefully within 5 seconds"""
    # Launch multiple agents
    agents = []
    for i in range(5):
        agent = agent_lifecycle_simulator.launch_single_agent(0, i + 1)
        assert agent is not None, f"Failed to launch agent {i+1}"
        agents.append(agent)

    # Test shutdown
    shutdown_times = []
    for agent in agents:
        start_time = time.time()
        success = agent_lifecycle_simulator.stop_agent(agent)
        shutdown_time = time.time() - start_time
        shutdown_times.append(shutdown_time)
        metrics_collector.record_shutdown_time(shutdown_time)

        assert success, f"Graceful shutdown failed for {agent['bot_id']}"
        assert shutdown_time < 5.0, f"Shutdown too slow: {shutdown_time:.1f}s"
        assert agent['process'].poll() is not None, "Process still running after shutdown"

    logger.info(f"Average shutdown time: {sum(shutdown_times)/len(shutdown_times):.2f}s")


@pytest.mark.lifecycle
def test_agent_memory_footprint_stability(agent_lifecycle_simulator, metrics_collector):
    """Test 3: Agent memory footprint must remain stable within 10% variance"""
    # Launch agent and monitor memory
    agent = agent_lifecycle_simulator.launch_single_agent(0, 5)
    assert agent is not None, "Agent launch failed"

    memory_readings = []
    process = agent['process']

    # Monitor memory for 30 seconds
    for _ in range(30):
        try:
            mem_info = psutil.Process(process.pid).memory_info()
            memory_mb = mem_info.rss / 1024 / 1024
            memory_readings.append(memory_mb)
            metrics_collector.record_memory_footprint(memory_mb)
        except psutil.NoSuchProcess:
            pytest.fail("Agent process died during memory monitoring")

        time.sleep(1)

    # Calculate variance
    if len(memory_readings) >= 10:
        avg_memory = sum(memory_readings) / len(memory_readings)
        max_memory = max(memory_readings)
        min_memory = min(memory_readings)

        variance_percent = ((max_memory - min_memory) / avg_memory) * 100
        assert variance_percent < 10.0, f"Memory variance too high: {variance_percent:.1f}%"

        logger.info(".1f")
        assert avg_memory < 500, f"Memory usage too high: {avg_memory:.1f}MB"


@pytest.mark.lifecycle
def test_agent_restart_recovery(agent_lifecycle_simulator, metrics_collector):
    """Test 4: Agents must restart successfully within 30 seconds after failure"""
    # Launch agent
    agent = agent_lifecycle_simulator.launch_single_agent(0, 6)
    assert agent is not None, "Initial launch failed"

    # Force failure by killing process
    agent['process'].kill()
    agent['process'].wait()

    # Attempt restart
    start_time = time.time()
    restart_agent = agent_lifecycle_simulator.launch_single_agent(0, 6)
    restart_time = time.time() - start_time

    metrics_collector.increment_restart_count()

    assert restart_agent is not None, "Restart launch failed"
    assert restart_time < 30.0, f"Restart time too slow: {restart_time:.1f}s"
    assert agent_lifecycle_simulator.check_agent_health(restart_agent), "Restarted agent not healthy"

    logger.info(".1f")


@pytest.mark.lifecycle
def test_concurrent_agent_operations(agent_lifecycle_simulator, metrics_collector):
    """Test 5: System must handle concurrent launches and shutdowns without conflicts"""
    # Test concurrent launches
    def launch_agent_worker(gpu_id: int, agent_id: int):
        agent = agent_lifecycle_simulator.launch_single_agent(gpu_id, agent_id + 7)
        return agent is not None and agent_lifecycle_simulator.check_agent_health(agent)

    # Launch 10 agents concurrently (simulating one GPU's worth)
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(launch_agent_worker, 0, i) for i in range(10)]
        results = [future.result() for future in futures]

    assert all(results), "Some concurrent launches failed"

    # Verify all agents are healthy
    healthy_count = sum(1 for config in agent_lifecycle_simulator.agent_configs[-10:]
                       if agent_lifecycle_simulator.check_agent_health(config))
    assert healthy_count == 10, f"Only {healthy_count}/10 agents healthy after concurrent launch"

    # Test concurrent shutdowns
    def shutdown_agent_worker(config: Dict[str, Any]):
        return agent_lifecycle_simulator.stop_agent(config)

    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(shutdown_agent_worker, config)
                  for config in agent_lifecycle_simulator.agent_configs[-10:]]
        results = [future.result() for future in futures]

    assert all(results), "Some concurrent shutdowns failed"

    logger.info("Concurrent operations completed successfully")


@pytest.mark.lifecycle
def test_agent_resource_isolation(agent_lifecycle_simulator):
    """Test 6: Agents must be isolated by GPU and not interfere with each other"""
    # Launch agents on different GPUs
    gpu0_agent = agent_lifecycle_simulator.launch_single_agent(0, 17)
    gpu1_agent = agent_lifecycle_simulator.launch_single_agent(1, 17)

    assert gpu0_agent is not None and gpu1_agent is not None, "Multi-GPU launch failed"
    assert gpu0_agent['gpu_id'] == 0, "GPU assignment incorrect for first agent"
    assert gpu1_agent['gpu_id'] == 1, "GPU assignment incorrect for second agent"

    # Check environment variables isolation
    assert gpu0_agent['env']['CUDA_VISIBLE_DEVICES'] == '0'
    assert gpu1_agent['env']['CUDA_VISIBLE_DEVICES'] == '1'
    assert gpu0_agent['env']['BOT_PORT'] != gpu1_agent['env']['BOT_PORT'], "Port conflict detected"

    logger.info("Resource isolation validated across GPUs")


@pytest.mark.lifecycle
def test_agent_lifecycle_state_persistence(agent_lifecycle_simulator, tmp_path):
    """Test 7: Agent state must persist across restarts"""
    # Launch agent with simulated state
    agent = agent_lifecycle_simulator.launch_single_agent(0, 18)
    assert agent is not None, "Agent launch failed"

    state_file = tmp_path / "agent_state_test.yaml"
    state_data = {'test_metric': 42, 'last_update': time.time()}

    # Simulate state save (in real implementation, this would be in agent's memory)
    with open(state_file, 'w') as f:
        yaml.dump(state_data, f)

    # Restart agent (kill and relaunch)
    agent_lifecycle_simulator.stop_agent(agent)

    # Reload state would happen in agent startup
    with open(state_file, 'r') as f:
        reloaded_state_raw = yaml.safe_load(f)
    if reloaded_state_raw is None:
        pytest.fail("Failed to load state")
    reloaded_state: Dict[str, Any] = cast(Dict[str, Any], reloaded_state_raw)
    assert reloaded_state['test_metric'] == 42, "State persistence failed"

    logger.info("State persistence validated")


# Integration tests with performance benchmarks
@pytest.mark.benchmark
@pytest.mark.lifecycle
def test_full_lifecycle_throughput(agent_lifecycle_simulator, benchmark):
    """Benchmark test: Complete lifecycle operations throughput"""
    def lifecycle_cycle():
        # Launch
        agent = agent_lifecycle_simulator.launch_single_agent(0, 99)
        health_ok = agent_lifecycle_simulator.check_agent_health(agent)

        # Quick operation
        time.sleep(0.1)

        # Shutdown
        agent_lifecycle_simulator.stop_agent(agent)

        return agent is not None and health_ok

    result = benchmark(lifecycle_cycle)
    assert result is True, "Lifecycle cycle failed"


# Validation summary - all tests must pass for Phase 2 completion
@pytest.mark.lifecycle
def test_lifecycle_validation_summary(agent_lifecycle_simulator, metrics_collector):
    """Final validation: All lifecycle requirements met"""
    # This test runs after all others and validates the entire suite
    total_agents_launched = len(agent_lifecycle_simulator.agent_configs)
    total_successful_launches = len([c for c in agent_lifecycle_simulator.agent_configs
                                   if agent_lifecycle_simulator.check_agent_health(c)])

    launch_time_avg = (sum(metrics_collector.metrics['launch_times']) /
                      len(metrics_collector.metrics['launch_times'])
                      if metrics_collector.metrics['launch_times'] else 0)

    # 100% green requirements
    assert total_agents_launched > 0, "No agents were launched during tests"
    assert total_successful_launches == total_agents_launched, f"Launch success rate: {total_successful_launches}/{total_agents_launched}"
    assert launch_time_avg < 0.100, f"Average launch time violation: {launch_time_avg:.3f}s"
    assert metrics_collector.metrics['restart_count'] >= 1, "Restart functionality not tested"

    logger.info("╔══════════════════════════════════════════════════════════╗")
    logger.info("║              LIFECYCLE VALIDATION SUMMARY                ║")
    logger.info("╠══════════════════════════════════════════════════════════╣")
    logger.info(f"║ Total Agents Launched:     {total_agents_launched:>3d}/100                     ║")
    logger.info(f"║ Successful Launches:       {total_successful_launches:>3d}/100                   ║")
    logger.info(f"║ Average Launch Time:       {launch_time_avg:>6.3f}s                   ║")
    logger.info(f"║ Restart Tests Performed:   {metrics_collector.metrics['restart_count']:>3d}                        ║")
    logger.info("╠══════════════════════════════════════════════════════════╣")
    logger.info("║                   ✅ ALL TESTS PASSED                    ║")
    logger.info("╚══════════════════════════════════════════════════════════╝")
